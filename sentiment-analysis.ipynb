{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhFC7m5_56qW"
   },
   "source": [
    "## Package Requirements\n",
    "\n",
    "To run the code, make sure the following packages are installed:\n",
    "\n",
    "- `pandas=1.5.3=py311heda8569_0`\n",
    "- `numpy=1.24.3=py311hdab7c0b_1`\n",
    "- `nltk=3.8.1=py311haa95532_0`\n",
    "- `scikit-learn=1.3.0=py311hf62ec03_0`\n",
    "- `scikit-learn-intelex=2023.1.1=py311haa95532_0`\n",
    "\n",
    "I used the Anaconda Python distribution, which typically comes with these packages pre-installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6m-wOEUi56qX",
    "outputId": "230fa3dc-a23b-4919-bf55-35f8208d266c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Negar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing/installing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRdCuKbd56qZ",
    "outputId": "96c95bc2-72ec-4aa9-8512-89f957dce692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\negar\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\negar\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\negar\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n"
     ]
    }
   ],
   "source": [
    "! pip install bs4 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC2eFUQr56qZ"
   },
   "source": [
    "## Read Data\n",
    "First we download data and save it to the same folder as the Jupyter notebook as data.tsv. Then we create a Pandas dataframe from our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0xozCiV656qZ",
    "outputId": "cd261d9b-8390-4f53-f991-6af00032f372"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Negar\\AppData\\Local\\Temp\\ipykernel_26516\\2023660551.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_table('data.tsv', delimiter='\\t', header=0, on_bad_lines='skip')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>52782374</td>\n",
       "      <td>R1PR37BR7G3M6A</td>\n",
       "      <td>B00D7H8XB6</td>\n",
       "      <td>868449945</td>\n",
       "      <td>AmazonBasics 12-Sheet High-Security Micro-Cut ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>24045652</td>\n",
       "      <td>R3BDDDZMZBZDPU</td>\n",
       "      <td>B001XCWP34</td>\n",
       "      <td>33521401</td>\n",
       "      <td>Derwent Colored Pencils, Inktense Ink Pencils,...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1          US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2          US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "3          US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
       "4          US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "3  AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
       "4  Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
       "\n",
       "  star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0           5            0.0          0.0    N                 Y   \n",
       "1           5            0.0          1.0    N                 Y   \n",
       "2           5            0.0          0.0    N                 Y   \n",
       "3           1            2.0          3.0    N                 Y   \n",
       "4           4            0.0          0.0    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                         Five Stars   \n",
       "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                      but I am sure I will like it.   \n",
       "3  and the shredder was dirty and the bin was par...   \n",
       "4                                         Four Stars   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                                     Great product.  2015-08-31  \n",
       "1  What's to say about this commodity item except...  2015-08-31  \n",
       "2    Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
       "3  Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
       "4                    Gorgeous colors and easy to use  2015-08-31  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the downloaded data as a table and creating dataframe \"df\"\n",
    "df = pd.read_table('data.tsv', delimiter='\\t', header=0, on_bad_lines='skip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OefV4buD56qZ"
   },
   "source": [
    "## Keep Reviews and Ratings\n",
    "To do sentiment analysis, we need to only keep reviews and ratings, sincce we are going to predict the sentiment class of a review. This is done by training our model with a bunch of reviews and their sentiment class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBPPlxq756qa",
    "outputId": "22cd0c7f-ed68-43c2-9a07-dbe15ed329f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  star_rating                                        review_body\n",
      "0           5                                     Great product.\n",
      "1           5  What's to say about this commodity item except...\n",
      "2           5    Haven't used yet, but I am sure I will like it.\n",
      "3           1  Although this was labeled as &#34;new&#34; the...\n",
      "4           4                    Gorgeous colors and easy to use\n"
     ]
    }
   ],
   "source": [
    "#Keeping only the review_body and star_rating columns\n",
    "\n",
    "df = df[['star_rating', 'review_body']]\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruaa5pUr56qa"
   },
   "source": [
    " ## We form two classes and select 50000 reviews randomly from each class.\n",
    "This is a binary classification problem, so we are going to form only two classes of sentiments (class 1 and 2). If the star rating of a review is 1, 2 or 3 we put it in class 1 (df_1) and rating of 4 and 5 as class 2(df_2). From each class, we use .sample method to randomly select 50000 reviews.The random_state parameter is used to initialize the random number generator, and we need to pass in an integer value to our random_state so that our results are erproducable each time we run the code. \n",
    ".We create a new column to represent the class in each dataframe and then remove the star_rating column (since we deal with class from now on). The average length of reviews in each class before data cleaning has been reported in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aHoN43pA56qa",
    "outputId": "5209cee3-091e-4a73-98f4-52b121b41035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  Class\n",
      "999431   Crappy color, fuzzy, it puts ink down in layer...      1\n",
      "1313016  These envelopes are of good quality however I ...      1\n",
      "2421335  I did not purchase this thru Amazon,     IT IS...      1\n",
      "20415    Returned it. Very cheap. Zipper wouldn't work....      1\n",
      "1225242  If you want to print lots of pics all at once,...      1\n",
      "2377357  Product works as described. It's certainly not...      1\n",
      "736148   Absolute piece of garbage. Better sharpening a...      1\n",
      "64340    There is hardly ANY tape on this roll! You nee...      1\n",
      "2205549  The light is too dim and not enough options to...      1\n",
      "573481   The cartridges were over filled and ink spilli...      1\n",
      "1739005  This may have been a random occurrence since t...      1\n",
      "501817   Broke within 2 weeks. Every time I plugged in ...      1\n",
      "1206175  Many times unable to read some of the letters/...      1\n",
      "1965712  This is my second - and last - HP printer.  DO...      1\n",
      "549869                                      a few leaks...      1\n",
      "1180602  description wasn't very clear and ended up not...      1\n",
      "906607   It's not even close to otter box but for the p...      1\n",
      "2132304  My 11 years old son wanted to buy these laser ...      1\n",
      "1712397  These were too big for a toddlers grip, may wo...      1\n",
      "1096659  Didn't hold staples, and didn't work at all,do...      1\n",
      "                                               review_body  Class\n",
      "1386897  I had a problem with the table not going up or...      2\n",
      "471488      Just what I was looking for...great price too.      2\n",
      "2341467  About a week ago, I purchased the Ooma Telo.  ...      2\n",
      "362686            On time delivery...product as described.      2\n",
      "13093    I *love* these paperclips. They hold so much b...      2\n",
      "31636         What a cool little device that works superb!      2\n",
      "980267   It's a sharpie.  If you have used sharpies bef...      2\n",
      "198680                                               Great      2\n",
      "495908                                         Works well!      2\n",
      "453294                                         its awesome      2\n",
      "583230                                           Excellent      2\n",
      "1775529  Our Elementary school theme this year is &#34;...      2\n",
      "2057201  Have used for a few months now and I love it!<...      2\n",
      "738888   Useful for gamers looking for an oversized sto...      2\n",
      "831621   Very nice printer.  Has all of the features I ...      2\n",
      "1171550                    Great product and shipped fast.      2\n",
      "14479          Great Product, hold up well and do the job!      2\n",
      "955103      Great price for better batts than the original      2\n",
      "2403364  Overall I am very happy with my Ooma experienc...      2\n",
      "1625855  This calendar is perfect if you are looking fo...      2\n",
      "Average length of class 1 reviews before data cleaning is 382.35086701734036\n",
      "Average length of class 2 reviews before data cleaning is 259.8337766755335\n"
     ]
    }
   ],
   "source": [
    "#splitting our data frame into class 1 and 2 based on star rating\n",
    "df_1 = df[(df['star_rating'] == 1) | (df['star_rating'] == 2) | (df['star_rating'] == 3)]\n",
    "df_2 = df[(df['star_rating'] == 4) | (df['star_rating'] == 5)]\n",
    "\n",
    "#randomly selecting 50000 reviews from each class\n",
    "df_1 = df_1.sample(n = 50000,random_state = 1)\n",
    "df_2 = df_2.sample(n = 50000, random_state = 1)\n",
    "\n",
    "# creating a new column for class in each data frame and dropping star_rating column\n",
    "df_1['Class'] = 1\n",
    "df_1.drop(columns=['star_rating'], inplace=True)\n",
    "\n",
    "df_2['Class'] = 2\n",
    "df_2.drop(columns=['star_rating'], inplace=True)\n",
    "\n",
    "print(df_1.head(20))\n",
    "print(df_2.head(20))\n",
    "\n",
    "#measuring length of review before data cleaning\n",
    "class_1_len_before_cleaning = df_1['review_body'].str.len().mean()\n",
    "class_2_len_before_cleaning = df_2['review_body'].str.len().mean()\n",
    "print(\"Average length of class 1 reviews before data cleaning is\",class_1_len_before_cleaning)\n",
    "print(\"Average length of class 2 reviews before data cleaning is\",class_2_len_before_cleaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xaofHIm56qa"
   },
   "source": [
    "# Data Cleaning\n",
    "\n",
    "The data cleaning is necessary to reduce noise and remove unnecessary information before starting analysis. In this step, we have used str.lower() method to turn all strings in review body into lower case. Then using str.replace() method, all url and html tags (that start with specific characters) have been replaced with an empty string. Then the str.strip() command has been used to remove any extra space. To remove contractions (e.g. it's -> it is) we have downloaded and used the contractions library(https://github.com/kootenpv/contractions) and a lambda function. Before that, we have to make sure that iff x is NaN (missing value) or not a string (e.g., if it's of another data type), it is replaced x with an empty string ''.This step ensures that all elements in the 'review_body' column are either valid strings or empty strings. In the final step, replace('[^a-zA-Z ]', '', regex=True) is used to replace characters in each element of the 'review_body' column based on a regular expression pattern.The regular expression [^a-zA-Z ] matches any character that is not a lowercase letter (a-z), an uppercase letter (A-Z), or a space. In other words, it matches any character that is not an alphabet letter or a space. By specifying regex=True, it treats the pattern as a regular expression. Average length of reviews in each class has been reported which is shorter than what if was before data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da3w2RASZkW4",
    "outputId": "a6d42bcc-1d38-4eb8-87b3-c19534d9c19c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Negar\\AppData\\Local\\Temp\\ipykernel_1532\\3355202267.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_1['review_body'] = df_1['review_body'].str.replace(r'http\\S+|www\\S+|https\\S+', '', case=False)\n",
      "C:\\Users\\Negar\\AppData\\Local\\Temp\\ipykernel_1532\\3355202267.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['review_body'] = df_2['review_body'].str.replace(r'http\\S+|www\\S+|https\\S+', '', case=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  Class\n",
      "999431   crappy color fuzzy it puts ink down in layers ...      1\n",
      "1313016  these envelopes are of good quality however i ...      1\n",
      "2421335  i did not purchase this thru amazon     it is ...      1\n",
      "20415    returned it very cheap zipper wouldnt work buy...      1\n",
      "1225242  if you want to print lots of pics all at once ...      1\n",
      "2377357  product works as described its certainly not f...      1\n",
      "736148   absolute piece of garbage better sharpening al...      1\n",
      "64340    there is hardly any tape on this roll you need...      1\n",
      "2205549  the light is too dim and not enough options to...      1\n",
      "573481   the cartridges were over filled and ink spilli...      1\n",
      "1739005  this may have been a random occurrence since t...      1\n",
      "501817   broke within  weeks every time i plugged in my...      1\n",
      "1206175  many times unable to read some of the lettersn...      1\n",
      "1965712  this is my second  and last  hp printer  do no...      1\n",
      "549869                                         a few leaks      1\n",
      "1180602  description wasnt very clear and ended up not ...      1\n",
      "906607   its not even close to otter box but for the pr...      1\n",
      "2132304  my  years old son wanted to buy these laser li...      1\n",
      "1712397  these were too big for a toddlers grip may wor...      1\n",
      "1096659  didnt hold staples and didnt work at alldo not...      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Negar\\AppData\\Local\\Temp\\ipykernel_1532\\3355202267.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_1['review_body'] = df_1['review_body'].str.replace(r'<.*?>', '')\n",
      "C:\\Users\\Negar\\AppData\\Local\\Temp\\ipykernel_1532\\3355202267.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_2['review_body'] = df_2['review_body'].str.replace(r'<.*?>', '')\n"
     ]
    }
   ],
   "source": [
    "# convert all reviews into lower case\n",
    "df_1['review_body'] = df_1['review_body'].str.lower()\n",
    "df_2['review_body'] = df_2['review_body'].str.lower()\n",
    "\n",
    "#remove urls from the reviews\n",
    "df_1['review_body'] = df_1['review_body'].str.replace(r'http\\S+|www\\S+|https\\S+', '', case=False)\n",
    "df_2['review_body'] = df_2['review_body'].str.replace(r'http\\S+|www\\S+|https\\S+', '', case=False)\n",
    "\n",
    "#remove html tags from the reviews\n",
    "df_1['review_body'] = df_1['review_body'].str.replace(r'<.*?>', '')\n",
    "df_2['review_body'] = df_2['review_body'].str.replace(r'<.*?>', '')\n",
    "\n",
    "#remove exra spaces \n",
    "df_1['review_body'] = df_1['review_body'].str.strip()\n",
    "df_2['review_body'] = df_2['review_body'].str.strip()\n",
    "\n",
    "print(df_1.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnDcAcYHl9uF",
    "outputId": "063fb903-260e-49c2-81fa-0b2287a9cca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\negar\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\negar\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\negar\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\negar\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "                                               review_body  Class\n",
      "999431   crappy color fuzzy it puts ink down in layers ...      1\n",
      "1313016  these envelopes are of good quality however i ...      1\n",
      "2421335  i did not purchase this thru amazon     it is ...      1\n",
      "20415    returned it very cheap zipper would not work b...      1\n",
      "1225242  if you want to print lots of pics all at once ...      1\n",
      "2377357  product works as described its certainly not f...      1\n",
      "736148   absolute piece of garbage better sharpening al...      1\n",
      "64340    there is hardly any tape on this roll you need...      1\n",
      "2205549  the light is too dim and not enough options to...      1\n",
      "573481   the cartridges were over filled and ink spilli...      1\n",
      "1739005  this may have been a random occurrence since t...      1\n",
      "501817   broke within  weeks every time i plugged in my...      1\n",
      "1206175  many times unable to read some of the lettersn...      1\n",
      "1965712  this is my second  and last  hp printer  do no...      1\n",
      "549869                                         a few leaks      1\n",
      "1180602  description was not very clear and ended up no...      1\n",
      "906607   its not even close to otter box but for the pr...      1\n",
      "2132304  my  years old son wanted to buy these laser li...      1\n",
      "1712397  these were too big for a toddlers grip may wor...      1\n",
      "1096659  did not hold staples and did not work at alldo...      1\n"
     ]
    }
   ],
   "source": [
    "#perform contractions on the reviews: https://github.com/kootenpv/contractions\n",
    "!pip install contractions\n",
    "import contractions\n",
    "#first we turn NaN (unidentified data) and non-string values into empty strings and then perform contractions on them\n",
    "df_1['review_body'] = df_1['review_body'].apply(lambda x: '' if pd.isna(x) or not isinstance(x, str) else x)\n",
    "df_1['review_body'] = df_1['review_body'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "df_2['review_body'] = df_2['review_body'].apply(lambda x: '' if pd.isna(x) or not isinstance(x, str) else x)\n",
    "df_2['review_body'] = df_2['review_body'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "print(df_1.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dUp3D5f6jXVP",
    "outputId": "202ea07f-337a-44a3-e2a9-8bbb25f679c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  Class\n",
      "999431   crappy color fuzzy it puts ink down in layers ...      1\n",
      "1313016  these envelopes are of good quality however i ...      1\n",
      "2421335  i did not purchase this thru amazon     it is ...      1\n",
      "20415    returned it very cheap zipper wouldnt work buy...      1\n",
      "1225242  if you want to print lots of pics all at once ...      1\n",
      "2377357  product works as described its certainly not f...      1\n",
      "736148   absolute piece of garbage better sharpening al...      1\n",
      "64340    there is hardly any tape on this roll you need...      1\n",
      "2205549  the light is too dim and not enough options to...      1\n",
      "573481   the cartridges were over filled and ink spilli...      1\n",
      "1739005  this may have been a random occurrence since t...      1\n",
      "501817   broke within  weeks every time i plugged in my...      1\n",
      "1206175  many times unable to read some of the lettersn...      1\n",
      "1965712  this is my second  and last  hp printer  do no...      1\n",
      "549869                                         a few leaks      1\n",
      "1180602  description wasnt very clear and ended up not ...      1\n",
      "906607   its not even close to otter box but for the pr...      1\n",
      "2132304  my  years old son wanted to buy these laser li...      1\n",
      "1712397  these were too big for a toddlers grip may wor...      1\n",
      "1096659  didnt hold staples and didnt work at alldo not...      1\n"
     ]
    }
   ],
   "source": [
    "# remove non-alphabetical characters\n",
    "df_1['review_body'] = df_1['review_body'].replace('[^a-zA-Z ]', '', regex=True)\n",
    "df_2['review_body'] = df_2['review_body'].replace('[^a-zA-Z ]', '', regex=True)\n",
    "\n",
    "print(df_1.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHRgkcvkxqHy",
    "outputId": "2e00a35c-7325-4328-b881-2fdad956721c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of class 1 reviews after data cleaning is 365.60226\n",
      "Average length of class 2 reviews after data cleaning is 248.64432\n"
     ]
    }
   ],
   "source": [
    "class_1_len_after_cleaning = df_1['review_body'].str.len().mean()\n",
    "class_2_len_after_cleaning = df_2['review_body'].str.len().mean()\n",
    "print(\"Average length of class 1 reviews after data cleaning is\",class_1_len_after_cleaning)\n",
    "print(\"Average length of class 2 reviews after data cleaning is\",class_2_len_after_cleaning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjSsXvYE56qa"
   },
   "source": [
    "# Pre-processing\n",
    "\n",
    "The first step of the preprocessing is to remove stop words (like \"a\", \"the\", \"is\" and \"are\").\n",
    "But before that, it is better to tokenize the reviews, meaning that we break it into its units. The word_tokenize function from NLTK is used for this purpose. We have also downloaded the list of English stopwords from NLTk and removed stop words using a lambda function. The lambda function filters out stopwords from each list of tokens in the 'review_body' column. It keeps only the words that are not in the stop_words list. In order to improve text analysis and understanding, we also need to perform lemmatization. lemmatization reduces words to their base or dictionary form. lemmatizer.lemmatize(word) is used to lemmatize each word in the list of tokens. We apply a lambda function to each element in the 'review_body' column of both data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnF0dR5O56qb",
    "outputId": "95619109-b38b-4852-9ad6-6119b77dd210"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Negar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Negar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of class 1 reviews before pre-processing is 69.52882\n",
      "Average length of class 2 reviews before pre-processing is 47.24498\n",
      "                                               review_body  Class\n",
      "999431   [crappy, color, fuzzy, puts, ink, layers, goes...      1\n",
      "1313016  [envelopes, good, quality, however, ordered, i...      1\n",
      "2421335  [purchase, thru, amazon, cheaper, walmart, sho...      1\n",
      "20415    [returned, cheap, zipper, would, work, buy, so...      1\n",
      "1225242  [want, print, lots, pics, works, great, howeve...      1\n",
      "2377357  [product, works, described, certainly, fancy, ...      1\n",
      "736148   [absolute, piece, garbage, better, sharpening,...      1\n",
      "64340    [hardly, tape, roll, need, equal, one, roll, r...      1\n",
      "2205549  [light, dim, enough, options, change, brightne...      1\n",
      "573481   [cartridges, filled, ink, spilling, good, qual...      1\n",
      "1739005  [may, random, occurrence, since, product, many...      1\n",
      "501817   [broke, within, weeks, every, time, plugged, i...      1\n",
      "1206175  [many, times, unable, read, lettersnumbers, ru...      1\n",
      "1965712  [second, last, hp, printer, buy, printer, revi...      1\n",
      "549869                                             [leaks]      1\n",
      "1180602  [description, clear, ended, fitting, daughters...      1\n",
      "906607   [even, close, otter, box, price, get, feel, go...      1\n",
      "2132304  [years, old, son, wanted, buy, laser, lights, ...      1\n",
      "1712397  [big, toddlers, grip, may, work, adultyou, nee...      1\n",
      "1096659  [hold, staples, work, alldo, buy, item, immedi...      1\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#tokenizing the reviews\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "df_1['review_body'] = df_1['review_body'].astype(str)\n",
    "df_2['review_body'] = df_2['review_body'].astype(str)\n",
    "\n",
    "df_1['review_body'] = df_1['review_body'].apply(word_tokenize)\n",
    "df_2['review_body'] = df_2['review_body'].apply(word_tokenize)\n",
    "\n",
    "#measuring length of class 1 and 2 reviews before preprocessing\n",
    "class_1_len_before_preprocessing = df_1['review_body'].str.len().mean()\n",
    "class_2_len_before_preprocessing = df_2['review_body'].str.len().mean()\n",
    "print(\"Average length of class 1 reviews before pre-processing is\",class_1_len_before_preprocessing)\n",
    "print(\"Average length of class 2 reviews before pre-processing is\",class_2_len_before_preprocessing)\n",
    "\n",
    "#we create a list of stop words and then remove all stop words from our review body\n",
    "stop_words = stopwords.words('english')\n",
    "df_1['review_body'] = df_1['review_body'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "df_2['review_body'] = df_2['review_body'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(df_1.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8G06Cgu56qb"
   },
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBz0kPLv56qb",
    "outputId": "6d79c12e-e5a0-4082-8059-ea0966fcc8a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  Class\n",
      "999431   [crappy, color, fuzzy, put, ink, layer, go, pr...      1\n",
      "1313016  [envelope, good, quality, however, ordered, in...      1\n",
      "2421335  [purchase, thru, amazon, cheaper, walmart, sho...      1\n",
      "20415    [returned, cheap, zipper, would, work, buy, so...      1\n",
      "1225242  [want, print, lot, pic, work, great, however, ...      1\n",
      "2377357  [product, work, described, certainly, fancy, g...      1\n",
      "736148   [absolute, piece, garbage, better, sharpening,...      1\n",
      "64340    [hardly, tape, roll, need, equal, one, roll, r...      1\n",
      "2205549  [light, dim, enough, option, change, brightnes...      1\n",
      "573481   [cartridge, filled, ink, spilling, good, quali...      1\n",
      "1739005  [may, random, occurrence, since, product, many...      1\n",
      "501817   [broke, within, week, every, time, plugged, ip...      1\n",
      "1206175  [many, time, unable, read, lettersnumbers, rui...      1\n",
      "1965712  [second, last, hp, printer, buy, printer, revi...      1\n",
      "549869                                              [leak]      1\n",
      "1180602  [description, clear, ended, fitting, daughter,...      1\n",
      "906607   [even, close, otter, box, price, get, feel, go...      1\n",
      "2132304  [year, old, son, wanted, buy, laser, light, ev...      1\n",
      "1712397  [big, toddler, grip, may, work, adultyou, need...      1\n",
      "1096659  [hold, staple, work, alldo, buy, item, immedia...      1\n"
     ]
    }
   ],
   "source": [
    "# Performing lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "df_1['review_body'] = df_1['review_body'].apply(lambda list:[lemmatizer.lemmatize(word) for word in list])\n",
    "df_2['review_body'] = df_2['review_body'].apply(lambda list:[lemmatizer.lemmatize(word) for word in list])\n",
    "print(df_1.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of class 1 reviews after data processing is 33.55024\n",
      "Average length of class 2 reviews after data processing is 23.4124\n"
     ]
    }
   ],
   "source": [
    "#measuring length of class 1 and 2 reviews after preprocessing\n",
    "class_1_len_after_preprocessing = df_1['review_body'].str.len().mean()\n",
    "class_2_len_after_preprocessing = df_2['review_body'].str.len().mean()\n",
    "print(\"Average length of class 1 reviews after data processing is\",class_1_len_after_preprocessing)\n",
    "print(\"Average length of class 2 reviews after data processing is\",class_2_len_after_preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiqJPxCI56qb"
   },
   "source": [
    "# TF-IDF and BoW Feature Extraction\n",
    "Before moving forward we concatenate and shuffle data from class 1 and class 2 to create a balanced dataset. The .sample method with a fraction of 1 does that for us. Then we split our data into 80% training data and 20% testing data using train_test_split from sklearn. In this step, x contains my features and y contains my lables(classes). Then we will do feature extraction on training and testing data separately. I had to set my max-features to 1000 since I ran out of memory when I didn't.\n",
    "Count Vectorizer does feature extraction based on Bag of Words method. The fit_transform() method is used during the training phase (bow_x_train = bow_vectorizer.fit_transform(x_train)) to both fit the bow_vectorizer to the training data (x_train) and transform it into a sparse matrix. The sparse matrix gives the frequency of words in our corpus (features) in each document (review).The transform() method is used during the testing phase (bow_x_test = bow_vectorizer.transform(x_test)) to apply the same transformation learned from the training data to the test data. This ensures that the same vocabulary (word features) used for training is applied to the test data, resulting in consistent feature representations.\n",
    "We repeat feature extraction using the TF_IDF method as well so that we can compare results from both features. This is done in a similar way for train and test data. The output of the Tfidf vectorization is the TF-IDF (Term Frequency-Inverse Document Frequency) matrix, which represents the importance of words our corpus, accounting for the fact that some words appear more frequently in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uPdawWqe56qb",
    "outputId": "08f3b8f3-946b-4dc0-f41e-c957aa84416b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               review_body  Class\n",
      "2598989  [psc, little, year, cutterhead, supposedly, ye...      1\n",
      "2323412  [pretty, solidly, built, cabinet, flimsy, thou...      2\n",
      "1764168  [placed, combo, order, prismacolor, premier, s...      1\n",
      "1210270  [perfect, replacement, fast, shipping, prior, ...      2\n",
      "1093602                                       [great, buy]      2\n"
     ]
    }
   ],
   "source": [
    "# concatenating data from class 1 and 2 and then shuffling them\n",
    "balanced_data = pd.concat([df_1, df_2])\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=1)\n",
    "\n",
    "print(balanced_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "Jy1VFMFwBlQK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 40)\t1\n",
      "  (0, 64)\t1\n",
      "  (0, 88)\t1\n",
      "  (0, 335)\t1\n",
      "  (0, 590)\t1\n",
      "  (0, 872)\t1\n",
      "  (0, 908)\t1\n",
      "  (1, 308)\t1\n",
      "  (1, 352)\t1\n",
      "  (1, 356)\t1\n",
      "  (1, 774)\t1\n",
      "  (1, 983)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 35)\t1\n",
      "  (2, 78)\t1\n",
      "  (2, 122)\t1\n",
      "  (2, 352)\t1\n",
      "  (2, 412)\t3\n",
      "  (2, 418)\t1\n",
      "  (2, 488)\t1\n",
      "  (2, 532)\t1\n",
      "  (2, 661)\t1\n",
      "  (2, 674)\t1\n",
      "  (2, 675)\t1\n",
      "  (2, 703)\t1\n",
      "  :\t:\n",
      "  (19999, 87)\t1\n",
      "  (19999, 101)\t1\n",
      "  (19999, 135)\t1\n",
      "  (19999, 162)\t1\n",
      "  (19999, 274)\t1\n",
      "  (19999, 352)\t1\n",
      "  (19999, 395)\t1\n",
      "  (19999, 403)\t1\n",
      "  (19999, 412)\t1\n",
      "  (19999, 450)\t1\n",
      "  (19999, 452)\t1\n",
      "  (19999, 466)\t1\n",
      "  (19999, 488)\t1\n",
      "  (19999, 552)\t1\n",
      "  (19999, 566)\t1\n",
      "  (19999, 573)\t1\n",
      "  (19999, 575)\t2\n",
      "  (19999, 657)\t2\n",
      "  (19999, 658)\t1\n",
      "  (19999, 818)\t1\n",
      "  (19999, 895)\t1\n",
      "  (19999, 903)\t1\n",
      "  (19999, 960)\t1\n",
      "  (19999, 989)\t1\n",
      "  (19999, 996)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 64)\t1\n",
      "  (0, 88)\t1\n",
      "  (0, 335)\t1\n",
      "  (0, 590)\t1\n",
      "  (0, 872)\t1\n",
      "  (0, 908)\t1\n",
      "  (1, 308)\t1\n",
      "  (1, 352)\t1\n",
      "  (1, 356)\t1\n",
      "  (1, 774)\t1\n",
      "  (1, 983)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 35)\t1\n",
      "  (2, 78)\t1\n",
      "  (2, 122)\t1\n",
      "  (2, 352)\t1\n",
      "  (2, 412)\t3\n",
      "  (2, 418)\t1\n",
      "  (2, 488)\t1\n",
      "  (2, 532)\t1\n",
      "  (2, 661)\t1\n",
      "  (2, 674)\t1\n",
      "  (2, 675)\t1\n",
      "  (2, 703)\t1\n",
      "  :\t:\n",
      "  (19999, 87)\t1\n",
      "  (19999, 101)\t1\n",
      "  (19999, 135)\t1\n",
      "  (19999, 162)\t1\n",
      "  (19999, 274)\t1\n",
      "  (19999, 352)\t1\n",
      "  (19999, 395)\t1\n",
      "  (19999, 403)\t1\n",
      "  (19999, 412)\t1\n",
      "  (19999, 450)\t1\n",
      "  (19999, 452)\t1\n",
      "  (19999, 466)\t1\n",
      "  (19999, 488)\t1\n",
      "  (19999, 552)\t1\n",
      "  (19999, 566)\t1\n",
      "  (19999, 573)\t1\n",
      "  (19999, 575)\t2\n",
      "  (19999, 657)\t2\n",
      "  (19999, 658)\t1\n",
      "  (19999, 818)\t1\n",
      "  (19999, 895)\t1\n",
      "  (19999, 903)\t1\n",
      "  (19999, 960)\t1\n",
      "  (19999, 989)\t1\n",
      "  (19999, 996)\t1\n"
     ]
    }
   ],
   "source": [
    "#Splitting data into 80% training data and 20% testing data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x, max_features = 1000)\n",
    "\n",
    "x = balanced_data['review_body']\n",
    "y = balanced_data['Class']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Bag of Words Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(analyzer=lambda x: x, max_features = 1000)\n",
    "\n",
    "bow_x_train = bow_vectorizer.fit_transform (x_train)\n",
    "bow_x_test = bow_vectorizer.transform(x_test)\n",
    "\n",
    "print(bow_x_test)\n",
    "print(bow_x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "PlBd2ITvCBLU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 602)\t0.6134238710039859\n",
      "  (0, 356)\t0.4776859525926707\n",
      "  (0, 499)\t0.6289096001637421\n",
      "  (1, 964)\t0.20493697448296408\n",
      "  (1, 509)\t0.2703759573757694\n",
      "  (1, 798)\t0.20543586282169735\n",
      "  (1, 920)\t0.1825865198205182\n",
      "  (1, 592)\t0.2685840745635208\n",
      "  (1, 674)\t0.19703140100231215\n",
      "  (1, 289)\t0.2017628236134194\n",
      "  (1, 635)\t0.24789170576434108\n",
      "  (1, 966)\t0.1494722654513359\n",
      "  (1, 281)\t0.2595991219704854\n",
      "  (1, 386)\t0.2838099994002555\n",
      "  (1, 532)\t0.18558720011288682\n",
      "  (1, 21)\t0.22996610304699885\n",
      "  (1, 787)\t0.5536764153141333\n",
      "  (1, 938)\t0.17908731401778002\n",
      "  (2, 333)\t0.17734902785217496\n",
      "  (2, 664)\t0.08723951504516353\n",
      "  (2, 94)\t0.14982025121408182\n",
      "  (2, 676)\t0.16875522035531876\n",
      "  (2, 984)\t0.11604163872358368\n",
      "  (2, 598)\t0.15884865040465349\n",
      "  (2, 359)\t0.15411417277389455\n",
      "  :\t:\n",
      "  (79999, 550)\t0.13793586177413716\n",
      "  (79999, 870)\t0.1173751622943249\n",
      "  (79999, 38)\t0.1722340541953556\n",
      "  (79999, 692)\t0.15904852696031457\n",
      "  (79999, 162)\t0.15115283437870666\n",
      "  (79999, 278)\t0.13067844416521895\n",
      "  (79999, 431)\t0.11794658811358891\n",
      "  (79999, 465)\t0.16707140130065093\n",
      "  (79999, 538)\t0.19922453047521035\n",
      "  (79999, 185)\t0.12525016224687344\n",
      "  (79999, 451)\t0.12531622547042573\n",
      "  (79999, 711)\t0.3072547575790852\n",
      "  (79999, 388)\t0.12793656609731405\n",
      "  (79999, 264)\t0.09747494211842028\n",
      "  (79999, 176)\t0.16152291595275053\n",
      "  (79999, 896)\t0.08343561545197278\n",
      "  (79999, 66)\t0.12461940310459871\n",
      "  (79999, 309)\t0.104724364849098\n",
      "  (79999, 933)\t0.0780764558447448\n",
      "  (79999, 340)\t0.08488596525775831\n",
      "  (79999, 892)\t0.12272700800953108\n",
      "  (79999, 575)\t0.0731691147886151\n",
      "  (79999, 651)\t0.2530192012996901\n",
      "  (79999, 87)\t0.1883577837122743\n",
      "  (79999, 532)\t0.10955051244193924\n",
      "  (0, 908)\t0.3837437983314672\n",
      "  (0, 872)\t0.39449563461603676\n",
      "  (0, 590)\t0.38871637929143815\n",
      "  (0, 335)\t0.4015701338862729\n",
      "  (0, 88)\t0.2735040392613402\n",
      "  (0, 64)\t0.3784042694226493\n",
      "  (0, 40)\t0.40836337365462455\n",
      "  (1, 983)\t0.2655396586038875\n",
      "  (1, 774)\t0.5649422491432105\n",
      "  (1, 356)\t0.2887303765556789\n",
      "  (1, 352)\t0.30168894929985834\n",
      "  (1, 308)\t0.6602631993121417\n",
      "  (2, 989)\t0.14485789485406877\n",
      "  (2, 984)\t0.2095850617962455\n",
      "  (2, 739)\t0.25373958623460313\n",
      "  (2, 703)\t0.21166108975067324\n",
      "  (2, 675)\t0.21551206985285798\n",
      "  (2, 674)\t0.2221115093544787\n",
      "  (2, 661)\t0.19128254883787843\n",
      "  (2, 532)\t0.20921057721891367\n",
      "  (2, 488)\t0.2626989858898792\n",
      "  (2, 417)\t0.2727031285883792\n",
      "  (2, 411)\t0.5099322560318074\n",
      "  (2, 352)\t0.15291182024775535\n",
      "  (2, 121)\t0.17602913322895886\n",
      "  :\t:\n",
      "  (19999, 960)\t0.16431489875704286\n",
      "  (19999, 903)\t0.19921571867537186\n",
      "  (19999, 895)\t0.25753465759719735\n",
      "  (19999, 818)\t0.2305301998467369\n",
      "  (19999, 658)\t0.17179252666438016\n",
      "  (19999, 657)\t0.24708190556934817\n",
      "  (19999, 575)\t0.21806956067517405\n",
      "  (19999, 573)\t0.17534565393561571\n",
      "  (19999, 566)\t0.243835435907897\n",
      "  (19999, 552)\t0.160162390742448\n",
      "  (19999, 488)\t0.2049870493070715\n",
      "  (19999, 465)\t0.12448280667421559\n",
      "  (19999, 451)\t0.18674309722590016\n",
      "  (19999, 449)\t0.22687644125610965\n",
      "  (19999, 411)\t0.13263534073211844\n",
      "  (19999, 402)\t0.23861539565470458\n",
      "  (19999, 395)\t0.17442882193027576\n",
      "  (19999, 352)\t0.11931885740091959\n",
      "  (19999, 274)\t0.22687644125610965\n",
      "  (19999, 161)\t0.2320468945104993\n",
      "  (19999, 134)\t0.21949779089903065\n",
      "  (19999, 101)\t0.13931526821388351\n",
      "  (19999, 87)\t0.14034302335948268\n",
      "  (19999, 31)\t0.16733019112546083\n",
      "  (19999, 21)\t0.2022867661047981\n"
     ]
    }
   ],
   "source": [
    "##TF_IDF feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer=lambda x: x, max_features = 1000)\n",
    "\n",
    "tfidf_x_train = tfidf_vectorizer.fit_transform (x_train)\n",
    "tfidf_x_test = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "print(tfidf_x_train)\n",
    "print(tfidf_x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYHdU-fr56qb"
   },
   "source": [
    "# Perceptron Using Both Features\n",
    "\n",
    "In this step, we train a Perceptron classifier using Bag of Words (BoW) features (bow_x_train) and corresponding labels (y_train). The same is done for Tf-Idf features(tfidf_x_train). Then we evaluate the model's performance on the test data (bow_x_test) and (tfidf_x_test) using metrics accuracy, precision, recall, and F1-score. These metrics provide insights into how well the classifier is performing in terms of correctly classifying instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "5Zdh4aT856qb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7712\n",
      "bow_ppn_precision: 0.7625146656237779 , bow_ppn_recall: 0.7841343253569274 , bow_ppn_f1: 0.7731733914940022\n"
     ]
    }
   ],
   "source": [
    "#let's train a model using sklearn implementation of perceptron\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#creating the perceptron instance(classifier)\n",
    "\n",
    "ppn_clf = Perceptron()\n",
    "\n",
    "#bow_features --------------------------\n",
    "#training the classifier\n",
    "\n",
    "ppn_clf.fit(bow_x_train, y_train)\n",
    "\n",
    "#makng perdictions\n",
    "bow_ppn_y_predict = ppn_clf.predict(bow_x_test)\n",
    "\n",
    "#measuring performance using accuracy score\n",
    "print(\"accuracy:\", accuracy_score(y_test, bow_ppn_y_predict))\n",
    "\n",
    "#performace using precision, recall and F1 score\n",
    "bow_ppn_precision = precision_score(y_test, bow_ppn_y_predict)\n",
    "bow_ppn_recall = recall_score(y_test, bow_ppn_y_predict)\n",
    "bow_ppn_f1 = f1_score(y_test, bow_ppn_y_predict)\n",
    "print(\"bow_ppn_precision:\", bow_ppn_precision, \", bow_ppn_recall:\", bow_ppn_recall, \", bow_ppn_f1:\", bow_ppn_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "EgDuPmhM56qb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.74315\n",
      "tfidf_ppn_precision: 0.802186753801684 , tfidf_ppn_recall: 0.641765533882968 , tfidf_ppn_f1_score: 0.7130648494665699\n"
     ]
    }
   ],
   "source": [
    "#tfidf_features --------------------------\n",
    "\n",
    "#training the classifier\n",
    "ppn_clf.fit(tfidf_x_train, y_train)\n",
    "\n",
    "#makng perdictions\n",
    "tfidf_ppn_y_predict = ppn_clf.predict(tfidf_x_test)\n",
    "\n",
    "#measuring performance using accuracy score\n",
    "print(\"accuracy:\", accuracy_score(y_test, tfidf_ppn_y_predict))\n",
    "\n",
    "#performace using precision, recall and F1 score\n",
    "#tfidf_ppn_precision, tfidf_ppn_recall, tfidf_ppn_f1, tfidf_ppn_support = precision_recall_fscore_support(y_test, y_predict)\n",
    "tfidf_ppn_precision = precision_score(y_test, tfidf_ppn_y_predict)\n",
    "tfidf_ppn_recall = recall_score(y_test, tfidf_ppn_y_predict)\n",
    "tfidf_ppn_f1 = f1_score(y_test, tfidf_ppn_y_predict)\n",
    "print(\"tfidf_ppn_precision:\", tfidf_ppn_precision, \", tfidf_ppn_recall:\", tfidf_ppn_recall, \", tfidf_ppn_f1_score:\", tfidf_ppn_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Using Both Features\n",
    "We trains an SVM classifier using both BoW and TF-IDF feature representations and evaluate the classifier's performance in terms of accuracy, precision, recall, and F1-score for each feature representation. This allows us to compare how well the SVM classifier performs with different text feature representations. The max_iter parameter is set to a large value (100,000) to ensure good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.82665\n",
      "bow_svm_precision: 0.8109820485744457 , bow_svm_recall: 0.8493866881158254 , bow_svm_f1_score: 0.8297402150960075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#creating the SVM instance (classifier)\n",
    "SVM_clf = SVC(max_iter = 100000)\n",
    "\n",
    "#bow_features --------------------------\n",
    "#training the classifier\n",
    "SVM_clf.fit(bow_x_train, y_train)\n",
    "\n",
    "\n",
    "#makng perdictions\n",
    "bow_svm_y_predict = SVM_clf.predict(bow_x_test)\n",
    "\n",
    "#measuring performance using accuracy score\n",
    "print(\"accuracy:\", accuracy_score(y_test, bow_svm_y_predict))\n",
    "\n",
    "\n",
    "#performace using precision, recall and F1 score\n",
    "bow_svm_precision = precision_score(y_test, bow_svm_y_predict)\n",
    "bow_svm_recall = recall_score(y_test, bow_svm_y_predict)\n",
    "bow_svm_f1 = f1_score(y_test, bow_svm_y_predict)\n",
    "print(\"bow_svm_precision:\", bow_svm_precision, \", bow_svm_recall:\", bow_svm_recall, \", bow_svm_f1_score:\", bow_svm_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8433\n",
      "tfidf_svm_precision: 0.8381652104845115 , tfidf_svm_recall: 0.8487834305248341 , tfidf_svm_f1_score: 0.8434409031871316\n"
     ]
    }
   ],
   "source": [
    "#tfidf_features --------------------------\n",
    "#training the classifier\n",
    "SVM_clf.fit(tfidf_x_train, y_train)\n",
    "\n",
    "\n",
    "#makng perdictions\n",
    "tfidf_svm_y_predict = SVM_clf.predict(tfidf_x_test)\n",
    "\n",
    "#measuring performance using accuracy score\n",
    "print(\"accuracy:\", accuracy_score(y_test, tfidf_svm_y_predict))\n",
    "\n",
    "\n",
    "#performace using precision, recall and F1 score\n",
    "#tfidf_svm_precision, tfidf_svm_recall, tfidf_svm_f1, tfidf_svm_support = precision_recall_fscore_support(y_test, tfidf_svm_y_predict, average= 'micro')\n",
    "tfidf_svm_precision = precision_score(y_test, tfidf_svm_y_predict)\n",
    "tfidf_svm_recall = recall_score(y_test, tfidf_svm_y_predict)\n",
    "tfidf_svm_f1 = f1_score(y_test, tfidf_svm_y_predict)\n",
    "print(\"tfidf_svm_precision:\", tfidf_svm_precision, \", tfidf_svm_recall:\", tfidf_svm_recall, \", tfidf_svm_f1_score:\", tfidf_svm_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3mLEgtI56qb"
   },
   "source": [
    "# Logistic Regression Using Both Features\n",
    "We train a Logistic Regression classifier with two different text features, Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF). After training, we evaluate the classifier's performance using metrics  accuracy, precision, recall, and F1-score for each feature. This approach enables us to compare and assess how well the Logistic Regression classifier performs when using different text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "7H7CWzL056qb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.82635\n",
      "bow_lr_precision: 0.8339696625735218 , bow_lr_recall: 0.8125879750653529 , bow_lr_f1_score: 0.8231399908336303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_clf = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "#bow_features --------------------------\n",
    "#training the classifier\n",
    "LR_clf.fit(bow_x_train, y_train)\n",
    "\n",
    "#makng perdictions\n",
    "bow_lr_y_predict = LR_clf.predict(bow_x_test)\n",
    "\n",
    "#measuring performance using accuracy score\n",
    "print(\"accuracy:\", accuracy_score(y_test, bow_lr_y_predict))\n",
    "\n",
    "#performace using precision, recall and F1 score\n",
    "bow_lr_precision = precision_score(y_test, bow_lr_y_predict)\n",
    "bow_lr_recall = recall_score(y_test, bow_lr_y_predict)\n",
    "bow_lr_f1 = f1_score(y_test, bow_lr_y_predict)\n",
    "print(\"bow_lr_precision:\", bow_lr_precision, \", bow_lr_recall:\", bow_lr_recall, \", bow_lr_f1_score:\", bow_lr_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8297\n",
      "tfidf_lr_precision: 0.8226124704025256 , tfidf_lr_recall: 0.8383269656143173 , tfidf_lr_f1_score: 0.8303953789463201\n"
     ]
    }
   ],
   "source": [
    "#tfidf_features --------------------------\n",
    "#training the classifier\n",
    "LR_clf.fit(tfidf_x_train, y_train)\n",
    "\n",
    "#makng perdictions\n",
    "tfidf_lr_y_predict = LR_clf.predict(tfidf_x_test)\n",
    "\n",
    "#measuring performance using accuracy score\n",
    "print(\"accuracy:\", accuracy_score(y_test, tfidf_lr_y_predict))\n",
    "\n",
    "#performace using precision, recall and F1 score\n",
    "tfidf_lr_precision = precision_score(y_test, tfidf_lr_y_predict)\n",
    "tfidf_lr_recall = recall_score(y_test, tfidf_lr_y_predict)\n",
    "tfidf_lr_f1 = f1_score(y_test, tfidf_lr_y_predict)\n",
    "print(\"tfidf_lr_precision:\", tfidf_lr_precision, \", tfidf_lr_recall:\", tfidf_lr_recall, \", tfidf_lr_f1_score:\", tfidf_lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsMpQNoJ56qb"
   },
   "source": [
    "# Naive Bayes Using Both Features\n",
    "In the final step, we train a Naive Bayes classifier using Bag of Words (BoW) features (bow_x_train) and corresponding labels (y_train). The same is done for Tf-Idf features(tfidf_x_train). We have used the Multinomial Naive Bayes for better scores. Then we evaluate the model's performance on the test data (bow_x_test) and (tfidf_x_test) using metrics accuracy, precision, recall, and F1-score. These metrics provide insights into how well the classifier is performing in terms of correctly classifying instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.78715\n",
      "bow_nb_precision: 0.8239380480583077 , bow_nb_recall: 0.7274281118037402 , bow_nb_f1_score: 0.7726811555508091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_clf = MultinomialNB()\n",
    "\n",
    "#bow_features --------------------------\n",
    "#training the classifier\n",
    "NB_clf.fit(bow_x_train.toarray(), y_train)\n",
    "\n",
    "#makng perdictions\n",
    "bow_nb_y_predict = NB_clf.predict(bow_x_test)\n",
    "\n",
    "\n",
    "#measuring performance using accuracy score\n",
    "print(\"accuracy:\", accuracy_score(y_test, bow_nb_y_predict))\n",
    "\n",
    "\n",
    "#performace using precision, recall and F1 score\n",
    "bow_nb_precision = precision_score(y_test, bow_nb_y_predict)\n",
    "bow_nb_recall = recall_score(y_test, bow_nb_y_predict)\n",
    "bow_nb_f1 = f1_score(y_test, bow_nb_y_predict)\n",
    "print(\"bow_nb_precision:\", bow_nb_precision, \", bow_nb_recall:\", bow_nb_recall, \", bow_nb_f1_score:\", bow_nb_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.80255\n",
      "tfidf_nb_precision: 0.8006015037593985 , tfidf_nb_recall: 0.8029358536094913 , tfidf_nb_f1_score: 0.8017669795692988\n"
     ]
    }
   ],
   "source": [
    "#tfidf_features --------------------------\n",
    "#training the classifier\n",
    "NB_clf.fit(tfidf_x_train.toarray(), y_train)\n",
    "\n",
    "#makng perdictions\n",
    "tfidf_nb_y_predict = NB_clf.predict(tfidf_x_test)\n",
    "\n",
    "#measuring performance using accuracy score\n",
    "print(\"accuracy:\", accuracy_score(y_test, tfidf_nb_y_predict))\n",
    "\n",
    "#performace using precision, recall and F1 score\n",
    "tfidf_nb_precision = precision_score(y_test, tfidf_nb_y_predict)\n",
    "tfidf_nb_recall = recall_score(y_test, tfidf_nb_y_predict)\n",
    "tfidf_nb_f1 = f1_score(y_test, tfidf_nb_y_predict)\n",
    "print(\"tfidf_nb_precision:\", tfidf_nb_precision, \", tfidf_nb_recall:\", tfidf_nb_recall, \", tfidf_nb_f1_score:\", tfidf_nb_f1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
